{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сначала повторим результаты семинара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GAME=\"Skiing-v0\"\n",
    "IMAGE_W,IMAGE_H = IMAGE_SIZE =(160,100)\n",
    "N_AGENTS = 10\n",
    "SEQ_LENGTH = 25\n",
    "\n",
    "from __future__ import print_function \n",
    "experiment_setup_name = \"tutorial.gym.atari.skiing.cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=\"floatX=float32\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pygame\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from scipy.misc import imresize\n",
    "%matplotlib inline\n",
    "%env THEANO_FLAGS=\"floatX=float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You asked for game Skiing-v0 but path /home/ubuntu/env/lib/python3.5/site-packages/atari_py/atari_roms/Skiing-v0.bin does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-114cc5278f1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#creating a game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0matari\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAtari\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maction_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_meanings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, frameskip, image_size, interpolation, grayscale, deflicker, deflicker_buffer_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAtari\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframeskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat_action_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/gym/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, obs_type, frameskip, repeat_action_probability)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matari_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_game_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You asked for game %s but path %s does not exist'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframeskip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: You asked for game Skiing-v0 but path /home/ubuntu/env/lib/python3.5/site-packages/atari_py/atari_roms/Skiing-v0.bin does not exist"
     ]
    }
   ],
   "source": [
    "from env import Atari\n",
    "\n",
    "#creating a game\n",
    "atari = Atari(GAME,image_size=IMAGE_SIZE) \n",
    "\n",
    "action_names = np.array(atari.get_action_meanings())\n",
    "\n",
    "obs = atari.step(0)[0]\n",
    "\n",
    "plt.imshow(obs,interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-26 16:03:06,476] Making new env: Skiing-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f75355be278>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALMAAAD8CAYAAAA8GpVKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wNcFL3iQtJndgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f75356606a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(obs):\n",
    "    return (imresize(obs,IMAGE_SIZE).mean(-1)/255.)\n",
    "\n",
    "atari = gym.make(GAME)\n",
    "atari.reset()\n",
    "action_names = np.array(atari.get_action_meanings())\n",
    "\n",
    "obs = atari.step(0)[0]\n",
    "\n",
    "plt.imshow(preprocess(obs),interpolation='none',cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using simple convolutional neural network.\n",
    "\n",
    "![scheme](https://s18.postimg.org/gbmsq6gmx/dqn_scheme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, DimshuffleLayer\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,IMAGE_W,IMAGE_H))\n",
    "\n",
    "#reshape to [sample, color, x, y] to allow for convolutional layers to work correctly\n",
    "# observation_reshape = DimshuffleLayer(observation_layer,(0,3,1,2))\n",
    "\n",
    "\n",
    "\n",
    "from agentnet.memory import WindowAugmentation,LSTMCell,RNNCell\n",
    "\n",
    "#store 4-tick window in order to perceive motion of objects\n",
    "\n",
    "prev_window = InputLayer((None,4,IMAGE_W,IMAGE_H))\n",
    "\n",
    "current_window = WindowAugmentation(observation_layer,prev_window)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from lasagne.layers import Conv2DLayer,Pool2DLayer,DenseLayer,batch_norm,dropout\n",
    "\n",
    "#main neural network body\n",
    "conv0 = Conv2DLayer(current_window,32,filter_size=(8,8),stride=(4,4),name='conv0')\n",
    "\n",
    "conv1 = Conv2DLayer(batch_norm(conv0),64,filter_size=(4,4),stride=(2,2),name='conv1')\n",
    "\n",
    "dense0 = DenseLayer(batch_norm(conv1),512,name='dense',nonlinearity = lasagne.nonlinearities.tanh)\n",
    "\n",
    "#please set this to your last layer for convenience\n",
    "last_layer = dense0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a layer that predicts Qvalues\n",
    "qvalues_layer = DenseLayer(last_layer,\n",
    "                   num_units = n_actions,\n",
    "                   nonlinearity=lasagne.nonlinearities.linear,\n",
    "                   name=\"q-evaluator layer\")\n",
    "\n",
    "#To pick actions, we use an epsilon-greedy resolver (epsilon is a property)\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer,name=\"e-greedy action picker\")\n",
    "\n",
    "action_layer.epsilon.set_value(np.float32(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.target_network import TargetNetwork\n",
    "targetnet = TargetNetwork(qvalues_layer,dense0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "qvalues_old = targetnet.output_layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "from lasagne.layers import Conv2DLayer,Pool2DLayer,DenseLayer,batch_norm,dropout\n",
    "\n",
    "cnn = Conv2DLayer(observation_reshape, 16, filter_size=(8,8),stride=(4,4),name='l0')\n",
    "cnn = Conv2DLayer(cnn, 32,filter_size=(5,5),stride=(4,4),name='l1')\n",
    "cnn = Conv2DLayer(cnn, 64,filter_size=(4,4),stride=(4,4),name='l2')\n",
    "dnn = DenseLayer(cnn, 256, nonlinearity=lasagne.nonlinearities.tanh, name='dense')\n",
    "last_layer = dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a layer that predicts Qvalues for all actions.\n",
    "# Just adense layer with corresponding number of units and no nonlinearity (lasagne.nonlinearity.linear)\n",
    "n_actions = atari.action_space.n\n",
    "qvalues_layer = DenseLayer(last_layer,\n",
    "                   num_units = n_actions,\n",
    "                   nonlinearity=lasagne.nonlinearities.linear,\n",
    "                   name=\"QEvaluator\")\n",
    "\n",
    "#To pick actions, we use an epsilon-greedy resolver (epsilon is a property)\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer,name=\"e-greedy action picker\")\n",
    "\n",
    "action_layer.epsilon.set_value(np.float32(0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=qvalues_layer,\n",
    "              action_layers=action_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=(qvalues_layer,qvalues_old),\n",
    "              agent_states={current_window:prev_window},\n",
    "              action_layers=action_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[conv0.W,\n",
       " conv0_bn.beta,\n",
       " conv0_bn.gamma,\n",
       " conv1.W,\n",
       " conv1_bn.beta,\n",
       " conv1_bn.gamma,\n",
       " dense.W,\n",
       " dense.b,\n",
       " q-evaluator layer.W,\n",
       " q-evaluator layer.b]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A thin wrapper for openAI gym environments that maintains a set of parallel games and has a method to generate\n",
    "interaction sessions given agent one-step applier function.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "from agentnet.utils.layers import get_layer_dtype\n",
    "from warnings import warn\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "\n",
    "\n",
    "def GamePool(*args, **kwargs):\n",
    "    raise ValueError(\"Deprecated. Use EnvPool(agent,env_title,n_parallel_agents) instead\")\n",
    "\n",
    "\n",
    "deprecated_preprocess_obs = lambda obs: obs\n",
    "\n",
    "# A whole lot of space invaders\n",
    "class EnvPool(object):\n",
    "    def __init__(self, agent, make_env=lambda: gym.make(\"skiing-v0\"), n_games=1, max_size=None,\n",
    "                 preprocess_observation=deprecated_preprocess_obs, agent_step=None):\n",
    "        \"\"\"A pool that stores several\n",
    "           - game states (gym environment)\n",
    "           - prev observations - last agent observations\n",
    "           - prev memory states - last agent hidden states\n",
    "        and is capable of some auxilary actions like evaluating agent on one game session (See .evaluate()).\n",
    "        :param agent: Agent which interacts with the environment.\n",
    "        :type agent: agent.Agent\n",
    "        :param make_env: Factory that produces environments OR a name of the gym environment.\n",
    "                See gym.envs.registry.all()\n",
    "        :type make_env: function or str\n",
    "        :param n_games: Number of parallel games. One game by default.\n",
    "        :type n_games: int\n",
    "        :param max_size: Max pool size by default (if appending sessions). By default, pool is not constrained in size.\n",
    "        :type max_size: int\n",
    "        :param preprocess_observation: Function for preprocessing raw observations from gym env to agent format.\n",
    "            By default it is identity function.\n",
    "        :type preprocess_observation: function\n",
    "        :param agent_step: Function with the same signature as agent.get_react_function().\n",
    "        :type agent_step: theano.function\n",
    "        \"\"\"\n",
    "        if not callable(make_env):\n",
    "            env_name = make_env\n",
    "            make_env = lambda: gym.make(env_name)\n",
    "\n",
    "        ##Deprecation warning\n",
    "        if preprocess_observation != deprecated_preprocess_obs:\n",
    "            warn(\"preprocess_observation is deprecated (will be removed in 0.11). Use gym.core.Wrapper instead.\")\n",
    "\n",
    "        # Create atari games.\n",
    "        self.make_env = make_env\n",
    "        self.envs = [self.make_env() for _ in range(n_games)]\n",
    "        self.preprocess_observation = preprocess_observation\n",
    "\n",
    "        # Initial observations.\n",
    "        self.prev_observations = [self.preprocess_observation(make_env.reset()) for make_env in self.envs]\n",
    "\n",
    "        # Agent memory variables (if you use recurrent networks).\n",
    "        self.prev_memory_states = [np.zeros((n_games,) + tuple(mem.output_shape[1:]),\n",
    "                                            dtype=get_layer_dtype(mem))\n",
    "                                   for mem in agent.agent_states]\n",
    "\n",
    "        # Save agent.\n",
    "        self.agent = agent\n",
    "        self.agent_step = agent_step or agent.get_react_function()\n",
    "\n",
    "        # Create experience replay environment.\n",
    "        self.experience_replay = SessionPoolEnvironment(observations=agent.observation_layers,\n",
    "                                                        actions=agent.action_layers,\n",
    "                                                        agent_memories=agent.agent_states)\n",
    "        self.max_size = max_size\n",
    "\n",
    "        # Whether particular session has just been terminated and needs restarting.\n",
    "        self.just_ended = [False] * len(self.envs)\n",
    "\n",
    "    def interact(self, n_steps=100, verbose=False, add_last_observation=True):\n",
    "        \"\"\"Generate interaction sessions with ataries (openAI gym atari environments)\n",
    "        Sessions will have length n_steps. Each time one of games is finished, it is immediately getting reset\n",
    "        and this time is recorded in is_alive_log (See returned values).\n",
    "        :param n_steps: Length of an interaction.\n",
    "        :param verbose: If True, prints small debug message whenever a game gets reloaded after end.\n",
    "        :param add_last_observation: If True, appends the final state with\n",
    "                state=final_state,\n",
    "                action=-1,\n",
    "                reward=0,\n",
    "                new_memory_states=prev_memory_states, effectively making n_steps-1 records.\n",
    "        :returns: observation_log, action_log, reward_log, [memory_logs], is_alive_log, info_log\n",
    "        :rtype: a bunch of tensors [batch, tick, size...],\n",
    "                the only exception is info_log, which is a list of infos for [time][batch], None padded tick\n",
    "        \"\"\"\n",
    "\n",
    "        def env_step(i, action):\n",
    "            \"\"\"Environment reaction.\n",
    "            :returns: observation, reward, is_alive, info\n",
    "            \"\"\"\n",
    "\n",
    "            if not self.just_ended[i]:\n",
    "                new_observation, cur_reward, is_done, info = self.envs[i].step(action)\n",
    "                if is_done:\n",
    "                    # Game ends now, will finalize on next tick.\n",
    "                    self.just_ended[i] = True\n",
    "                new_observation = self.preprocess_observation(new_observation)\n",
    "\n",
    "                # note: is_alive=True in any case because environment is still alive (last tick alive) in our notation.\n",
    "                return new_observation, cur_reward, True, info\n",
    "            else:\n",
    "                # Reset environment, get new observation to be used on next tick.\n",
    "                new_observation = self.preprocess_observation(self.envs[i].reset())\n",
    "\n",
    "                # Reset memory for new episode.\n",
    "                for m_i in range(len(new_memory_states)):\n",
    "                    new_memory_states[m_i][i] = 0\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"env %i reloaded\" % i)\n",
    "\n",
    "                self.just_ended[i] = False\n",
    "\n",
    "                return new_observation, 0, False, {'end': True}\n",
    "\n",
    "        history_log = []\n",
    "\n",
    "        for i in range(n_steps - int(add_last_observation)):\n",
    "            res = self.agent_step(self.prev_observations, *self.prev_memory_states)\n",
    "            actions, new_memory_states = res[0], res[1:]\n",
    "\n",
    "            new_observations, cur_rewards, is_alive, infos = zip(*map(env_step, range(len(self.envs)), actions))\n",
    "\n",
    "            # Append data tuple for this tick.\n",
    "            history_log.append((self.prev_observations, actions, cur_rewards, new_memory_states, is_alive, infos))\n",
    "\n",
    "            self.prev_observations = new_observations\n",
    "            self.prev_memory_states = new_memory_states\n",
    "\n",
    "        if add_last_observation:\n",
    "            fake_actions = np.array([env.action_space.sample() for env in self.envs])\n",
    "            fake_rewards = np.zeros(shape=len(self.envs))\n",
    "            fake_is_alive = np.ones(shape=len(self.envs))\n",
    "            history_log.append((self.prev_observations, fake_actions, fake_rewards, self.prev_memory_states,\n",
    "                                fake_is_alive, [None] * len(self.envs)))\n",
    "\n",
    "        # cast to numpy arrays\n",
    "        observation_log, action_log, reward_log, memories_log, is_alive_log, info_log = zip(*history_log)\n",
    "\n",
    "        # tensor dimensions\n",
    "        # [batch_i, time_i, observation_size...]\n",
    "        observation_log = np.array(observation_log).swapaxes(0, 1)\n",
    "\n",
    "        # [batch, time, units] for each memory tensor\n",
    "        memories_log = list(map(lambda mem: np.array(mem).swapaxes(0, 1), zip(*memories_log)))\n",
    "\n",
    "        # [batch_i,time_i]\n",
    "        action_log = np.array(action_log).swapaxes(0, 1)\n",
    "\n",
    "        # [batch_i, time_i]\n",
    "        reward_log = np.array(reward_log).swapaxes(0, 1)\n",
    "\n",
    "        # [batch_i, time_i]\n",
    "        is_alive_log = np.array(is_alive_log).swapaxes(0, 1).astype('uint8')\n",
    "\n",
    "        return observation_log, action_log, reward_log, memories_log, is_alive_log, info_log\n",
    "\n",
    "    def update(self, n_steps=100, append=False, max_size=None, add_last_observation=True,\n",
    "               preprocess=lambda observations, actions, rewards, is_alive, h0: (\n",
    "                       observations, actions, rewards, is_alive, h0)):\n",
    "        \"\"\"Create new sessions and add them into the pool.\n",
    "        :param n_steps: How many time steps in each session.\n",
    "        :param append: If True, appends sessions to the pool and crops at max_size.\n",
    "            Otherwise, old sessions will be thrown away entirely.\n",
    "        :param max_size: If not None, substitutes default max_size (from __init__) for this update only.\n",
    "        :param add_last_observation: See param `add_last_observation` in `.interact()` method.\n",
    "        :param preprocess: Function that implements arbitrary processing of the sessions.\n",
    "            Takes AND outputs (observation_tensor, action_tensor, reward_tensor, is_alive_tensor, preceding_memory_states).\n",
    "            For param specs see `.interact()` output format.\n",
    "        \"\"\"\n",
    "\n",
    "        preceding_memory_states = list(self.prev_memory_states)\n",
    "\n",
    "        # Get interaction sessions.\n",
    "        observation_tensor, action_tensor, reward_tensor, _, is_alive_tensor, _ = self.interact(n_steps=n_steps,\n",
    "                                                                                                add_last_observation=add_last_observation)\n",
    "\n",
    "        observation_tensor, action_tensor, reward_tensor, is_alive_tensor, preceding_memory_states = \\\n",
    "            preprocess(observation_tensor, action_tensor, reward_tensor, is_alive_tensor, preceding_memory_states)\n",
    "\n",
    "        # Load them into experience replay environment.\n",
    "        if not append:\n",
    "            self.experience_replay.load_sessions(observation_tensor, action_tensor, reward_tensor,\n",
    "                                                 is_alive_tensor, preceding_memory_states)\n",
    "        else:\n",
    "            self.experience_replay.append_sessions(observation_tensor, action_tensor, reward_tensor,\n",
    "                                                   is_alive_tensor, preceding_memory_states,\n",
    "                                                   max_pool_size=max_size or self.max_size)\n",
    "\n",
    "    def evaluate(self, n_games=1, save_path=\"./records\", use_monitor=True, record_video=True, verbose=True,\n",
    "                 t_max=10000):\n",
    "        \"\"\"Plays an entire game start to end, records the logs(and possibly mp4 video), returns reward.\n",
    "        :param save_path: where to save the report\n",
    "        :param record_video: if True, records mp4 video\n",
    "        :return: total reward (scalar)\n",
    "        \"\"\"\n",
    "        env = self.make_env()\n",
    "\n",
    "        if not use_monitor and record_video:\n",
    "            raise warn(\"Cannot video without gym monitor. If you still want video, set use_monitor to True\")\n",
    "\n",
    "        if record_video :\n",
    "            env = Monitor(env,save_path,force=True)\n",
    "        elif use_monitor:\n",
    "            env = Monitor(env, save_path, video_callable=lambda i: False, force=True)\n",
    "\n",
    "        game_rewards = []\n",
    "        for _ in range(n_games):\n",
    "            # initial observation\n",
    "            observation = env.reset()\n",
    "            # initial memory\n",
    "            prev_memories = [np.zeros((1,) + tuple(mem.output_shape[1:]),\n",
    "                                      dtype=get_layer_dtype(mem))\n",
    "                             for mem in self.agent.agent_states]\n",
    "\n",
    "            t = 0\n",
    "            total_reward = 0\n",
    "            while True:\n",
    "\n",
    "                res = self.agent_step(self.preprocess_observation(observation)[None, ...], *prev_memories)\n",
    "                action, new_memories = res[0], res[1:]\n",
    "\n",
    "                observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "                total_reward += reward\n",
    "                prev_memories = new_memories\n",
    "\n",
    "                if done or t >= t_max:\n",
    "                    if verbose:\n",
    "                        print(\"Episode finished after {} timesteps with reward={}\".format(t + 1, total_reward))\n",
    "                    break\n",
    "                t += 1\n",
    "            game_rewards.append(total_reward)\n",
    "\n",
    "        env.close()\n",
    "        del env\n",
    "        return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A thin wrapper for openAI gym environments that maintains a set of parallel games and has a method to generate interaction sessions\n",
    "given agent one-step applier function\n",
    "\"\"\"\n",
    "\n",
    "from env import Atari\n",
    "import numpy as np\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "from agentnet.utils.layers import get_layer_dtype\n",
    "\n",
    "# A whole lot of space invaders\n",
    "class AtariGamePool(object):\n",
    "    def __init__(self, agent,game_title, n_games,max_size=None, **kwargs):\n",
    "        \"\"\"\n",
    "        A pool that stores several\n",
    "           - game states (gym environment)\n",
    "           - prev_observations - last agent observations\n",
    "           - prev memory states - last agent hidden states\n",
    "\n",
    "        :param game_title: name of the game. See here http://yavar.naddaf.name/ale/list_of_current_games.html\n",
    "        :param n_games: number of parallel games\n",
    "        :param kwargs: options passed to Atari when creating a game. See Atari __init__\n",
    "        \"\"\"\n",
    "        #create atari games\n",
    "        self.game_kwargs = kwargs\n",
    "        self.game_title = game_title\n",
    "        self.games = [Atari(self.game_title,**self.game_kwargs) for _ in range(n_games)]\n",
    "\n",
    "        #initial observations\n",
    "        self.prev_observations = [atari.reset() for atari in self.games]\n",
    "\n",
    "        #agent memory variables (if you use recurrent networks\n",
    "        self.prev_memory_states = [np.zeros((n_games,)+tuple(mem.output_shape[1:]),\n",
    "                                   dtype=get_layer_dtype(mem))\n",
    "                         for mem in agent.agent_states]\n",
    "\n",
    "        #save agent\n",
    "        self.agent = agent\n",
    "        self.agent_step = agent.get_react_function()\n",
    "\n",
    "        # Create experience replay environment\n",
    "        self.experience_replay = SessionPoolEnvironment(observations=agent.observation_layers,\n",
    "                                                        actions=agent.action_layers,\n",
    "                                                        agent_memories=agent.agent_states)\n",
    "        self.max_size = max_size\n",
    "\n",
    "\n",
    "\n",
    "    def interact(self, n_steps=100, verbose=False):\n",
    "        \"\"\"generate interaction sessions with ataries (openAI gym atari environments)\n",
    "        Sessions will have length n_steps.\n",
    "        Each time one of games is finished, it is immediately getting reset\n",
    "\n",
    "\n",
    "        params:\n",
    "            agent_step: a function(observations,memory_states) -> actions,new memory states for agent update\n",
    "            n_steps: length of an interaction\n",
    "            verbose: if True, prints small debug message whenever a game gets reloaded after end.\n",
    "        returns:\n",
    "            observation_log,action_log,reward_log,[memory_logs],is_alive_log,info_log\n",
    "            a bunch of tensors [batch, tick, size...]\n",
    "\n",
    "            the only exception is info_log, which is a list of infos for [time][batch]\n",
    "        \"\"\"\n",
    "        history_log = []\n",
    "        for i in range(n_steps):\n",
    "            res = self.agent_step(self.prev_observations, *self.prev_memory_states)\n",
    "            actions, new_memory_states = res[0],res[1:]\n",
    "\n",
    "            new_observations, cur_rewards, is_done, infos = \\\n",
    "                zip(*map(\n",
    "                    lambda atari, action: atari.step(action),\n",
    "                    self.games,\n",
    "                    actions)\n",
    "                    )\n",
    "\n",
    "            new_observations = np.array(new_observations)\n",
    "\n",
    "            for i in range(len(self.games)):\n",
    "                if is_done[i]:\n",
    "                    new_observations[i] = self.games[i].reset()\n",
    "\n",
    "                    for m_i in range(len(new_memory_states)):\n",
    "                        new_memory_states[m_i][i] = 0\n",
    "\n",
    "                    if verbose:\n",
    "                        print(\"atari %i reloaded\" % i)\n",
    "\n",
    "            # append observation -> action -> reward tuple\n",
    "            history_log.append((self.prev_observations, actions, cur_rewards, new_memory_states, is_done, infos))\n",
    "\n",
    "            self.prev_observations = new_observations\n",
    "            self.prev_memory_states = new_memory_states\n",
    "\n",
    "        # cast to numpy arrays\n",
    "        observation_log, action_log, reward_log, memories_log, is_done_log, info_log = zip(*history_log)\n",
    "\n",
    "        # tensor dimensions\n",
    "        # [batch_i, time_i, observation_size...]\n",
    "        observation_log = np.array(observation_log).swapaxes(0, 1)\n",
    "\n",
    "        # [batch, time, units] for each memory tensor\n",
    "        memories_log = map(lambda mem: np.array(mem).swapaxes(0, 1), zip(*memories_log))\n",
    "\n",
    "        # [batch_i,time_i]\n",
    "        action_log = np.array(action_log).swapaxes(0, 1)\n",
    "\n",
    "        # [batch_i, time_i]\n",
    "        reward_log = np.array(reward_log).swapaxes(0, 1)\n",
    "\n",
    "        # [batch_i, time_i]\n",
    "        is_alive_log = 1 - np.array(is_done_log, dtype='int8').swapaxes(0, 1)\n",
    "\n",
    "\n",
    "        return observation_log, action_log, reward_log, memories_log, is_alive_log, info_log\n",
    "\n",
    "\n",
    "    def update(self,n_steps=100,append=False,max_size=None):\n",
    "        \"\"\" a function that creates new sessions and ads them into the pool\n",
    "        throwing the old ones away entirely for simplicity\"\"\"\n",
    "\n",
    "        preceding_memory_states = list(self.prev_memory_states)\n",
    "\n",
    "        # get interaction sessions\n",
    "        observation_tensor, action_tensor, reward_tensor, _, is_alive_tensor, _ = self.interact(n_steps=n_steps)\n",
    "\n",
    "        # load them into experience replay environment\n",
    "        if not append:\n",
    "            self.experience_replay.load_sessions(observation_tensor, action_tensor, reward_tensor,\n",
    "                                                 is_alive_tensor, preceding_memory_states)\n",
    "        else:\n",
    "            self.experience_replay.append_sessions(observation_tensor, action_tensor, reward_tensor,\n",
    "                                                 is_alive_tensor, preceding_memory_states,\n",
    "                                                   max_pool_size=max_size or self.max_size)\n",
    "\n",
    "\n",
    "    def evaluate(self,n_games=1,save_path=\"./records\", record_video=True,verbose=True,t_max=10000):\n",
    "        \"\"\"\n",
    "        Plays an entire game start to end, records the logs(and possibly mp4 video), returns reward\n",
    "        :param save_path: where to save the report\n",
    "        :param record_video: if True, records mp4 video\n",
    "        :return: total reward (scalar)\n",
    "        \"\"\"\n",
    "        env = Atari(self.game_title, **self.game_kwargs)\n",
    "\n",
    "        if record_video :\n",
    "            env = Monitor(env,save_path,force=True)\n",
    "        elif use_monitor:\n",
    "            env = Monitor(env, save_path, video_callable=lambda i: False, force=True)\n",
    "\n",
    "        game_rewards = []\n",
    "        for _ in range(n_games):\n",
    "            # initial observation\n",
    "            observation = env.reset()\n",
    "            # initial memory\n",
    "            prev_memories = [np.zeros((1,) + tuple(mem.output_shape[1:]),\n",
    "                                      dtype=get_layer_dtype(mem))\n",
    "                             for mem in self.agent.agent_states]\n",
    "\n",
    "            t = 0\n",
    "            total_reward = 0\n",
    "            while True:\n",
    "\n",
    "                res = self.agent_step(observation[None,...], *prev_memories)\n",
    "                action, new_memories = res[0],res[1:]\n",
    "\n",
    "                observation, reward, done, info = env.step(action[0])\n",
    "                total_reward += reward\n",
    "                prev_memories = new_memories\n",
    "\n",
    "                if done or t >= t_max:\n",
    "                    if verbose:\n",
    "                        print(\"Episode finished after {} timesteps with reward={}\".format(t + 1,total_reward))\n",
    "                    break\n",
    "                t += 1\n",
    "            game_rewards.append(total_reward)\n",
    "\n",
    "        env.monitor.close()\n",
    "        del env\n",
    "        return np.mean(game_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pool import AtariGamePool\n",
    "pool = AtariGamePool(agent,GAME, N_AGENTS,image_size=IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AgentNet/agentnet/experiments/openai_gym/pool.py:58: UserWarning: preprocess_observation is deprecated (will be removed in 0.11). Use gym.core.Wrapper instead.\n",
      "  warn(\"preprocess_observation is deprecated (will be removed in 0.11). Use gym.core.Wrapper instead.\")\n",
      "[2017-01-26 16:12:03,018] Making new env: Skiing-v0\n",
      "[2017-01-26 16:12:03,064] Making new env: Skiing-v0\n",
      "[2017-01-26 16:12:03,102] Making new env: Skiing-v0\n",
      "[2017-01-26 16:12:03,140] Making new env: Skiing-v0\n",
      "[2017-01-26 16:12:03,180] Making new env: Skiing-v0\n",
      "[2017-01-26 16:12:03,216] Making new env: Skiing-v0\n",
      "[2017-01-26 16:12:03,245] Making new env: Skiing-v0\n",
      "[2017-01-26 16:12:03,289] Making new env: Skiing-v0\n",
      "[2017-01-26 16:12:03,331] Making new env: Skiing-v0\n",
      "[2017-01-26 16:12:03,367] Making new env: Skiing-v0\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir/lock\n",
      "[2017-01-26 16:12:05,360] Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "\n",
    "pool = EnvPool(agent, lambda: gym.make(GAME), \n",
    "               preprocess_observation=preprocess,\n",
    "               n_games=N_AGENTS,max_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-26 16:09:58,980] Making new env: Skiing-v0\n",
      "[2017-01-26 16:09:59,023] Making new env: Skiing-v0\n",
      "[2017-01-26 16:09:59,065] Making new env: Skiing-v0\n",
      "[2017-01-26 16:09:59,107] Making new env: Skiing-v0\n",
      "[2017-01-26 16:09:59,149] Making new env: Skiing-v0\n",
      "[2017-01-26 16:09:59,194] Making new env: Skiing-v0\n",
      "[2017-01-26 16:09:59,238] Making new env: Skiing-v0\n",
      "[2017-01-26 16:09:59,279] Making new env: Skiing-v0\n",
      "[2017-01-26 16:09:59,320] Making new env: Skiing-v0\n",
      "[2017-01-26 16:09:59,362] Making new env: Skiing-v0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-30a5a2222d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_AGENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-21b99487991c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, agent, make_env, n_games, max_size, preprocess_observation, agent_step)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Save agent.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_step\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_react_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Create experience replay environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/AgentNet/agentnet/agent/mdp_agent.py\u001b[0m in \u001b[0;36mget_react_function\u001b[0;34m(self, output_flags, function_flags)\u001b[0m\n\u001b[1;32m    532\u001b[0m         applier_fun = theano.function(applier_observations + list(applier_memories.values()),\n\u001b[1;32m    533\u001b[0m                                       \u001b[0mapplier_actions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mapplier_new_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m                                       **function_flags)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;31m# return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/compile/function.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/compile/pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                    \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m             defaults)\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input_storage, trustme, storage_map)\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             _fn, _i, _o = self.linker.make_thunk(\n\u001b[0;32m-> 1651\u001b[0;31m                 input_storage=input_storage_lists, storage_map=storage_map)\n\u001b[0m\u001b[1;32m   1652\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/gof/link.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m    697\u001b[0m         return self.make_all(input_storage=input_storage,\n\u001b[1;32m    698\u001b[0m                              \u001b[0moutput_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                              storage_map=storage_map)[:3]\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/gof/vm.py\u001b[0m in \u001b[0;36mmake_all\u001b[0;34m(self, profiler, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                                                  \u001b[0mcompute_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                                                  \u001b[0mno_recycling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m                                                  impl=impl))\n\u001b[0m\u001b[1;32m   1064\u001b[0m                 \u001b[0mlinker_make_thunk_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mthunk_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lazy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling, impl)\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 return self.make_c_thunk(node, storage_map, compute_map,\n\u001b[0;32m--> 924\u001b[0;31m                                          no_recycling)\n\u001b[0m\u001b[1;32m    925\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodNotDefined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 \u001b[0;31m# We requested the c code, so don't catch the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mmake_c_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trying CLinker.make_thunk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         outputs = cl.make_thunk(input_storage=node_input_storage,\n\u001b[0;32m--> 828\u001b[0;31m                                 output_storage=node_output_storage)\n\u001b[0m\u001b[1;32m    829\u001b[0m         \u001b[0mfill_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_output_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         cthunk, in_storage, out_storage, error_storage = self.__compile__(\n\u001b[1;32m   1189\u001b[0m             \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CThunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcthunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36m__compile__\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                                     \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                                     \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                                     keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return (thunk,\n\u001b[1;32m   1133\u001b[0m                 [link.Container(input, storage) for input, storage in\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36mcthunk_factory\u001b[0;34m(self, error_storage, in_storage, out_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1587\u001b[0m                 \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             module = get_module_cache().module_from_key(\n\u001b[0;32m-> 1589\u001b[0;31m                 key=key, lnk=self, keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m         \u001b[0mvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morphans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/gof/cmodule.py\u001b[0m in \u001b[0;36mmodule_from_key\u001b[0;34m(self, key, lnk, keep_lock)\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlimport_workdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlnk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_cmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36mcompile_cmodule\u001b[0;34m(self, location)\u001b[0m\n\u001b[1;32m   1490\u001b[0m                 \u001b[0mlib_dirs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mlibs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlibs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m                 preargs=preargs)\n\u001b[0m\u001b[1;32m   1493\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/sandbox/cuda/nvcc_compiler.py\u001b[0m in \u001b[0;36mcompile_str\u001b[0;34m(module_name, src_code, location, include_dirs, lib_dirs, libs, preargs, rpaths, py_module, hide_symbols)\u001b[0m\n\u001b[1;32m    348\u001b[0m             p = subprocess.Popen(\n\u001b[1;32m    349\u001b[0m                 cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mnvcc_stdout_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnvcc_stderr_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mconsole_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mnvcc_stdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnvcc_stdout_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsole_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1713\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pool = EnvPool(agent,lambda: gym.make(GAME), N_AGENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['RIGHT' 'NOOP' 'RIGHT' 'LEFT' 'LEFT' 'LEFT' 'NOOP']\n",
      " ['RIGHT' 'RIGHT' 'LEFT' 'LEFT' 'NOOP' 'LEFT' 'RIGHT']\n",
      " ['RIGHT' 'RIGHT' 'LEFT' 'LEFT' 'LEFT' 'NOOP' 'NOOP']\n",
      " ['RIGHT' 'RIGHT' 'LEFT' 'LEFT' 'LEFT' 'LEFT' 'RIGHT']\n",
      " ['RIGHT' 'RIGHT' 'LEFT' 'LEFT' 'LEFT' 'LEFT' 'RIGHT']\n",
      " ['RIGHT' 'RIGHT' 'LEFT' 'LEFT' 'LEFT' 'LEFT' 'LEFT']\n",
      " ['RIGHT' 'RIGHT' 'LEFT' 'LEFT' 'LEFT' 'LEFT' 'NOOP']\n",
      " ['RIGHT' 'RIGHT' 'LEFT' 'RIGHT' 'LEFT' 'LEFT' 'LEFT']\n",
      " ['RIGHT' 'RIGHT' 'LEFT' 'LEFT' 'LEFT' 'LEFT' 'NOOP']\n",
      " ['RIGHT' 'RIGHT' 'LEFT' 'LEFT' 'LEFT' 'LEFT' 'NOOP']]\n",
      "[[-3. -4. -3. -5. -3. -7.  0.]\n",
      " [-7. -3. -3. -5. -4. -6.  0.]\n",
      " [-3. -7. -5. -7. -6. -7.  0.]\n",
      " [-3. -5. -7. -5. -5. -7.  0.]\n",
      " [-5. -5. -5. -5. -3. -4.  0.]\n",
      " [-3. -4. -5. -3. -5. -3.  0.]\n",
      " [-5. -3. -5. -5. -7. -3.  0.]\n",
      " [-3. -5. -4. -5. -5. -5.  0.]\n",
      " [-3. -4. -3. -3. -7. -3.  0.]\n",
      " [-3. -4. -5. -5. -3. -5.  0.]]\n",
      "CPU times: user 276 ms, sys: 16 ms, total: 292 ms\n",
      "Wall time: 293 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "print(action_names[action_log])\n",
    "print(reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (Q-learning objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/AgentNet/agentnet/agent/mdp_agent.py:144: UserWarning: optimize_experience_replay is deprecated and will be removed in 1.0.2. Use experience_replay parameter.\n",
      "  warn(\"optimize_experience_replay is deprecated and will be removed in 1.0.2. Use experience_replay parameter.\")\n"
     ]
    }
   ],
   "source": [
    "#get agent's Qvalues obtained via experience replay.\n",
    "\n",
    "#This is an \"environment\" that replays all stored sessions.\n",
    "# replay = pool.experience_replay\n",
    "\n",
    "#To only sample several random sessions, try\n",
    "replay = pool.experience_replay.sample_session_batch(100,replace=True)\n",
    "\n",
    "_,_,_,_,qvalues_seq = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    optimize_experience_replay=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InplaceDimShuffle{1,0,2}.0, InplaceDimShuffle{1,0,2}.0]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qvalues_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2\n",
    "\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq[0],\n",
    "                                                      replay.actions[0],\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      gamma_or_gammas=0.99,)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates. Replace with any optimizer you want\n",
    "updates = lasagne.updates.adam(loss,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir/lock\n",
      "[2017-01-26 16:16:42,749] Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir/lock\n",
      "[2017-01-26 16:22:55,960] Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir/lock\n",
      "[2017-01-26 16:23:56,807] Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir/lock\n",
      "[2017-01-26 16:24:58,590] Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "#compile train function\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_epsilon(epoch_counter):\n",
    "    return 0.05 + 0.45*np.exp(-epoch_counter/1000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-26 16:29:43,514] Making new env: Skiing-v0\n",
      "[2017-01-26 16:29:43,571] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-26 16:29:43,572] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-01-26 16:29:43,590] Starting new video recorder writing to /home/ubuntu/records/openaigym.video.0.27462.video000000.mp4\n",
      "[2017-01-26 16:30:39,543] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/ubuntu/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 5976 timesteps with reward=-30000.0\n"
     ]
    }
   ],
   "source": [
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_counter = 1\n",
    "rewards = {epoch_counter:untrained_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 2560000 bytes of device memory (out of memory).\nApply node that caused the error: GpuFromHost(input)\nToposort index: 14\nInputs types: [TensorType(float32, 4D)]\nInputs shapes: [(10, 4, 160, 100)]\nInputs strides: [(256000, 64000, 400, 4)]\nInputs values: ['not shown']\nOutputs clients: [[GpuSubtensor{::, :int64:}(GpuFromHost.0, Constant{-1})]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Error allocating 2560000 bytes of device memory (out of memory).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-e87ef9104e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m##update resolver's epsilon (chance of random action instead of optimal one)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcurrent_epsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/AgentNet/agentnet/experiments/openai_gym/pool.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n_steps, append, max_size, add_last_observation, preprocess)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Get interaction sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         observation_tensor, action_tensor, reward_tensor, _, is_alive_tensor, _ = self.interact(n_steps=n_steps,\n\u001b[0;32m--> 194\u001b[0;31m                                                                                                 add_last_observation=add_last_observation)\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mobservation_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_alive_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreceding_memory_states\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/AgentNet/agentnet/experiments/openai_gym/pool.py\u001b[0m in \u001b[0;36minteract\u001b[0;34m(self, n_steps, verbose, add_last_observation)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_last_observation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_memory_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_memory_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    887\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/gof/link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Error allocating 2560000 bytes of device memory (out of memory).\nApply node that caused the error: GpuFromHost(input)\nToposort index: 14\nInputs types: [TensorType(float32, 4D)]\nInputs shapes: [(10, 4, 160, 100)]\nInputs strides: [(256000, 64000, 400, 4)]\nInputs values: ['not shown']\nOutputs clients: [[GpuSubtensor{::, :int64:}(GpuFromHost.0, Constant{-1})]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    pool.update(SEQ_LENGTH)\n",
    "    loss = train_step()\n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    current_epsilon = get_epsilon(epoch_counter)\n",
    "    action_layer.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    if epoch_counter%10==0:\n",
    "        print(\"iter=%i\\tepsilon=%.3f\"%(epoch_counter,current_epsilon))\n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%100 ==0:\n",
    "        rewards[epoch_counter] = pool.evaluate(record_video=False)\n",
    "        \n",
    "        plt.title(\"random frames\")\n",
    "        for i in range(min((len(pool.games),6))):\n",
    "            plt.subplot(2,3,i+1)\n",
    "            plt.imshow(pool.games[i].get_observation())\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gym.monitoring.monitor:Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\n",
      "[2016-10-29 00:04:30,455] Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\n",
      "INFO:gym.monitoring.monitor:Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2016-10-29 00:04:30,456] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "INFO:gym.monitoring.video_recorder:Starting new video recorder writing to /notebooks/Fold2/records/openaigym.video.31.8158.video000000.mp4\n",
      "[2016-10-29 00:04:30,507] Starting new video recorder writing to /notebooks/Fold2/records/openaigym.video.31.8158.video000000.mp4\n",
      "INFO:gym.monitoring.monitor:Finished writing results. You can upload them to the scoreboard via gym.upload('/notebooks/Fold2/records')\n",
      "[2016-10-29 00:04:43,613] Finished writing results. You can upload them to the scoreboard via gym.upload('/notebooks/Fold2/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1961 timesteps with reward=800.0\n",
      "mean session score=800.000000.5\n"
     ]
    }
   ],
   "source": [
    "rw = pool.evaluate(n_games=1,save_path=\"./records\",record_video=True)\n",
    "print(\"mean session score=%f.5\"%rw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Улучшение результата: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/yandexdataschool/AgentNet/blob/master/docs/user/whats_what.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent memory cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "('Error allocating 8192000 bytes of device memory (out of memory).', \"you might consider using 'theano.shared(..., borrow=True)'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-c3e616eef5ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2DLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropoutLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dropout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/lasagne/layers/dense.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, incoming, num_units, W, b, nonlinearity, num_leading_axes, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mnum_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_leading_axes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/lasagne/layers/base.py\u001b[0m in \u001b[0;36madd_param\u001b[0;34m(self, spec, shape, name, **tags)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s.%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# create shared variable, or pass through given variable/expression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;31m# parameters should be trainable and regularizable by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainable'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/lasagne/utils.py\u001b[0m in \u001b[0;36mcreate_param\u001b[0;34m(spec, shape, name)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m# broadcastable dimensions of expressions involving these parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mbcast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcastable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbcast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/compile/sharedvalue.py\u001b[0m in \u001b[0;36mshared\u001b[0;34m(value, name, strict, allow_downcast, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 var = ctor(value, name=name, strict=strict,\n\u001b[0;32m--> 272\u001b[0;31m                            allow_downcast=allow_downcast, **kwargs)\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_tag_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/env/lib/python3.5/site-packages/theano/sandbox/cuda/var.py\u001b[0m in \u001b[0;36mfloat32_shared_constructor\u001b[0;34m(value, name, strict, allow_downcast, borrow, broadcastable, target)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# type.broadcastable is guaranteed to be a tuple, which this next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# function requires\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mdeviceval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_support_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcastable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: ('Error allocating 8192000 bytes of device memory (out of memory).', \"you might consider using 'theano.shared(..., borrow=True)'\")"
     ]
    }
   ],
   "source": [
    "from lasagne.layers import InputLayer,DropoutLayer,DenseLayer, ExpressionLayer, Conv2DLayer,MaxPool2DLayer\n",
    "from lasagne.layers import flatten, dimshuffle\n",
    "\n",
    "observation_layer = InputLayer((None,IMAGE_W,IMAGE_H,3))\n",
    "observation_reshape = DimshuffleLayer(observation_layer,(0,3,1,2))\n",
    "observation_small = lasagne.layers.Pool2DLayer(observation_reshape,(2,2),mode='average_exc_pad')\n",
    "\n",
    "cnn = Conv2DLayer(observation_small, num_filters=64, filter_size=(8,8), stride=(4,4))\n",
    "cnn = Conv2DLayer(cnn, num_filters=128, filter_size=(4,4), stride=(2,2))\n",
    "\n",
    "dnn = DenseLayer(cnn,num_units=500)\n",
    "dnn = DropoutLayer(dnn,name = \"dropout\", p=0.05)\n",
    "dnn = DenseLayer(dnn,num_units=500)\n",
    "last_layer = dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.memory import WindowAugmentation, GRUMemoryLayer\n",
    "\n",
    "window_size = 3\n",
    "prev_window = InputLayer((None,window_size,last_layer.output_shape[1]))\n",
    "window = WindowAugmentation(last_layer, prev_window)\n",
    "\n",
    "gru_size = 256\n",
    "prev_gru = InputLayer((None,gru_size))\n",
    "\n",
    "window_max = ExpressionLayer(window,function=lambda v: v.max(axis=1), output_shape=last_layer.output_shape)\n",
    "window_max = lasagne.layers.FeaturePoolLayer(window,window_size)\n",
    "\n",
    "gru = GRUMemoryLayer(gru_size, observation_input = window_max, prev_state_input = prev_gru)\n",
    "\n",
    "from collections import OrderedDict\n",
    "memory_dict = OrderedDict([(window,prev_window), (gru,prev_gru)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Critic\n",
    "state_value_layer = DenseLayer(gru,\n",
    "                               num_units = 1,\n",
    "                               nonlinearity = lasagne.nonlinearities.linear)\n",
    "n_actions = atari.action_space.n\n",
    "#Actor\n",
    "policy_layer_pre_softmax = DenseLayer(gru,\n",
    "                                     num_units = n_actions,\n",
    "                                     nonlinearity= None)\n",
    "\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "policy_layer = NonlinearityLayer(policy_layer_pre_softmax,\n",
    "                                 lasagne.nonlinearities.softmax)\n",
    "\n",
    "\n",
    "#probabilistic resolver instead of greedy\n",
    "from agentnet.resolver import ProbabilisticResolver\n",
    "action_layer = ProbabilisticResolver(policy_layer,assume_normalized=True,name=\"resolver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              agent_states=memory_dict,\n",
    "              policy_estimators=[policy_layer,state_value_layer],\n",
    "              action_layers=action_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W,\n",
       " b,\n",
       " W,\n",
       " b,\n",
       " W,\n",
       " b,\n",
       " W,\n",
       " b,\n",
       " YetAnotherGRUMemoryLayer.W_in_to_updategate,\n",
       " YetAnotherGRUMemoryLayer.W_hid_to_updategate,\n",
       " YetAnotherGRUMemoryLayer.b_updategate,\n",
       " YetAnotherGRUMemoryLayer.W_in_to_resetgate,\n",
       " YetAnotherGRUMemoryLayer.W_hid_to_resetgate,\n",
       " YetAnotherGRUMemoryLayer.b_resetgate,\n",
       " YetAnotherGRUMemoryLayer.W_in_to_hidden_update,\n",
       " YetAnotherGRUMemoryLayer.W_hid_to_hidden_update,\n",
       " YetAnotherGRUMemoryLayer.b_hidden_update,\n",
       " W,\n",
       " b,\n",
       " W,\n",
       " b]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = lasagne.layers.get_all_params((action_layer,state_value_layer),trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: KungFuMaster-v0\n",
      "[2016-10-30 19:16:12,790] Making new env: KungFuMaster-v0\n",
      "INFO:gym.envs.registration:Making new env: KungFuMaster-v0\n",
      "[2016-10-30 19:16:12,830] Making new env: KungFuMaster-v0\n",
      "INFO:gym.envs.registration:Making new env: KungFuMaster-v0\n",
      "[2016-10-30 19:16:12,865] Making new env: KungFuMaster-v0\n",
      "INFO:gym.envs.registration:Making new env: KungFuMaster-v0\n",
      "[2016-10-30 19:16:12,904] Making new env: KungFuMaster-v0\n",
      "INFO:gym.envs.registration:Making new env: KungFuMaster-v0\n",
      "[2016-10-30 19:16:12,939] Making new env: KungFuMaster-v0\n",
      "INFO:gym.envs.registration:Making new env: KungFuMaster-v0\n",
      "[2016-10-30 19:16:12,974] Making new env: KungFuMaster-v0\n",
      "INFO:gym.envs.registration:Making new env: KungFuMaster-v0\n",
      "[2016-10-30 19:16:13,015] Making new env: KungFuMaster-v0\n",
      "INFO:gym.envs.registration:Making new env: KungFuMaster-v0\n",
      "[2016-10-30 19:16:13,055] Making new env: KungFuMaster-v0\n",
      "INFO:gym.envs.registration:Making new env: KungFuMaster-v0\n",
      "[2016-10-30 19:16:13,094] Making new env: KungFuMaster-v0\n",
      "INFO:gym.envs.registration:Making new env: KungFuMaster-v0\n",
      "[2016-10-30 19:16:13,129] Making new env: KungFuMaster-v0\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import GamePool\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "\n",
    "pool = GamePool(GAME, N_AGENTS)\n",
    "replay = SessionPoolEnvironment(observations = observation_layer, actions=action_layer, agent_memories=agent.agent_states.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "applier_fun = agent.get_react_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step(observation, prev_memories='zeros', batch_size=N_AGENTS):\n",
    "    if prev_memories == 'zeros':\n",
    "        prev_memories = [np.zeros((batch_size,) + tuple(mem.output_shape[1:]), dtype='float32') for mem in agent.agent_states]\n",
    "    res = applier_fun(np.array(observation), *prev_memories)\n",
    "    action = res[0]\n",
    "    memories = res[1:]\n",
    "    return action, memories\n",
    "\n",
    "def update_pool(replay, pool,n_steps=100):\n",
    "    preceding_memory_states = list(pool.prev_memory_states)\n",
    "    observation_tensor,action_tensor,reward_tensor,_,is_alive_tensor,_= pool.interact(step, n_steps)\n",
    "    replay.load_sessions(observation_tensor,action_tensor,reward_tensor,is_alive_tensor,preceding_memory_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "update_pool(replay,pool,SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_,observation_seq,_,_,(policy_seq,V_seq) = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    batch_size=replay.batch_size,\n",
    "    optimize_experience_replay=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.learning import a2c_n_step\n",
    "\n",
    "elwise_a2c_loss = a2c_n_step.get_elementwise_objective(policy_seq,\n",
    "                                                       V_seq[:,:,0],\n",
    "                                                       replay.actions[0],\n",
    "                                                       replay.rewards,\n",
    "                                                       replay.is_alive,\n",
    "                                                       n_steps=10, \n",
    "                                                       gamma_or_gammas=0.99,)\n",
    "\n",
    "loss = elwise_a2c_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updates = lasagne.updates.adam(loss,weights)\n",
    "session_reward = replay.rewards.sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([],[loss,session_reward],updates=updates)\n",
    "evaluation_fun = theano.function([],[loss,session_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10,loss 431.88685, rewards: 200.00000 \n",
      "epoch 20,loss -0.08556, rewards: 0.00000 \n",
      "epoch 30,loss -0.09492, rewards: 0.00000 \n",
      "epoch 40,loss 309.50071, rewards: 200.00000 \n",
      "epoch 50,loss 241.00919, rewards: 300.00000 \n",
      "epoch 60,loss 342.18519, rewards: 200.00000 \n",
      "epoch 70,loss 2255.60240, rewards: 1100.00000 \n",
      "epoch 80,loss 610.66371, rewards: 400.00000 \n",
      "epoch 90,loss 1632.83382, rewards: 400.00000 \n",
      "epoch 100,loss 4.03219, rewards: 0.00000 \n",
      "epoch 110,loss 918.83187, rewards: 200.00000 \n",
      "epoch 120,loss 3243.20667, rewards: 1000.00000 \n",
      "epoch 130,loss 3426.49851, rewards: 600.00000 \n",
      "epoch 140,loss 3073.33055, rewards: 800.00000 \n",
      "epoch 150,loss 1553.22035, rewards: 800.00000 \n",
      "epoch 160,loss 3073.05422, rewards: 1000.00000 \n",
      "epoch 170,loss 940.14464, rewards: 400.00000 \n",
      "epoch 180,loss 3792.28277, rewards: 1200.00000 \n",
      "epoch 190,loss 2114.12099, rewards: 600.00000 \n",
      "epoch 200,loss 2877.44725, rewards: 600.00000 \n",
      "epoch 210,loss 1411.33620, rewards: 400.00000 \n",
      "epoch 220,loss 1915.75600, rewards: 800.00000 \n",
      "epoch 230,loss 49.32303, rewards: 0.00000 \n",
      "epoch 240,loss 5501.01235, rewards: 1000.00000 \n",
      "epoch 250,loss 1893.42888, rewards: 800.00000 \n",
      "epoch 260,loss 2969.86453, rewards: 1200.00000 \n",
      "epoch 270,loss 2976.52369, rewards: 1000.00000 \n",
      "epoch 280,loss 2846.45702, rewards: 600.00000 \n",
      "epoch 290,loss 2079.00658, rewards: 600.00000 \n",
      "epoch 300,loss 817.74548, rewards: 600.00000 \n",
      "epoch 310,loss 1628.97019, rewards: 600.00000 \n",
      "epoch 320,loss 856.69499, rewards: 400.00000 \n",
      "epoch 330,loss 1001.00144, rewards: 400.00000 \n",
      "epoch 340,loss 4116.55404, rewards: 1200.00000 \n",
      "epoch 350,loss 2009.23799, rewards: 600.00000 \n",
      "epoch 360,loss 2497.32180, rewards: 600.00000 \n",
      "epoch 370,loss 1323.52089, rewards: 400.00000 \n",
      "epoch 380,loss 1006.83549, rewards: 200.00000 \n",
      "epoch 390,loss 1134.48477, rewards: 200.00000 \n",
      "epoch 400,loss 3212.43845, rewards: 800.00000 \n",
      "epoch 410,loss 3503.85926, rewards: 1000.00000 \n",
      "epoch 420,loss 650.06756, rewards: 200.00000 \n",
      "epoch 430,loss 23.54449, rewards: 0.00000 \n",
      "epoch 440,loss 19.89585, rewards: 0.00000 \n",
      "epoch 450,loss 1784.94531, rewards: 800.00000 \n",
      "epoch 460,loss 747.97735, rewards: 400.00000 \n",
      "epoch 470,loss 2868.41486, rewards: 800.00000 \n",
      "epoch 480,loss 3005.65081, rewards: 600.00000 \n",
      "epoch 490,loss 9402.14517, rewards: 1800.00000 \n",
      "epoch 500,loss 6522.77459, rewards: 1800.00000 \n",
      "epoch 510,loss 1233.15502, rewards: 200.00000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-effea8505293>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mupdate_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0maction_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavg_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m##record current learning progress and show learning curves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch_counter\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 700\n",
    "batch_size= 10\n",
    "epoch_counter = 1\n",
    "alpha = 0.1\n",
    "ma_reward_current = 0.\n",
    "ma_reward_greedy = 0.\n",
    "\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    #train\n",
    "    update_pool(replay,pool,SEQ_LENGTH)\n",
    "    action_layer.rng.seed(i)    \n",
    "    loss,avg_reward = train_fun()\n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%10 ==0:\n",
    "        ##update learning curves\n",
    "        full_loss, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "\n",
    "        print(\"epoch %i,loss %.5f, rewards: %.5f \"%(\n",
    "            epoch_counter,full_loss,avg_reward))\n",
    "\n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon.set_value(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "[m.close() for m in gym.monitoring._open_monitors()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: KungFuMaster-v0\n",
      "[2016-10-30 18:51:39,407] Making new env: KungFuMaster-v0\n",
      "INFO:gym.monitoring.monitor:Creating monitor directory ./records2/\n",
      "[2016-10-30 18:51:39,469] Creating monitor directory ./records2/\n",
      "INFO:gym.monitoring.video_recorder:Starting new video recorder writing to /notebooks/Fold2/records2/openaigym.video.3.9510.video000000.mp4\n",
      "[2016-10-30 18:51:39,549] Starting new video recorder writing to /notebooks/Fold2/records2/openaigym.video.3.9510.video000000.mp4\n",
      "INFO:gym.monitoring.video_recorder:Starting new video recorder writing to /notebooks/Fold2/records2/openaigym.video.3.9510.video000001.mp4\n",
      "[2016-10-30 18:53:13,469] Starting new video recorder writing to /notebooks/Fold2/records2/openaigym.video.3.9510.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 2149 timesteps\n",
      "Episode finished after 2186 timesteps\n",
      "Episode finished after 1837 timesteps\n",
      "Episode finished after 1755 timesteps\n",
      "Episode finished after 1786 timesteps\n",
      "Episode finished after 2605 timesteps\n",
      "Episode finished after 2179 timesteps\n",
      "Episode finished after 1901 timesteps"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.monitoring.video_recorder:Starting new video recorder writing to /notebooks/Fold2/records2/openaigym.video.3.9510.video000008.mp4\n",
      "[2016-10-30 18:59:41,440] Starting new video recorder writing to /notebooks/Fold2/records2/openaigym.video.3.9510.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode finished after 1856 timesteps\n",
      "Episode finished after 2013 timesteps"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.monitoring.monitor:Finished writing results. You can upload them to the scoreboard via gym.upload('/notebooks/Fold2/records2')\n",
      "[2016-10-30 19:01:30,159] Finished writing results. You can upload them to the scoreboard via gym.upload('/notebooks/Fold2/records2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_path = './records2/'\n",
    "step = agent.get_react_function()\n",
    "subm_env = gym.make(GAME)\n",
    "subm_env.monitor.start(save_path,force=True)\n",
    "\n",
    "for i_episode in xrange(10):\n",
    "    observation = subm_env.reset()\n",
    "    prev_memories = [np.zeros((1,) + tuple(mem.output_shape[1:]),\n",
    "                              dtype=getattr(mem,\"output_dtype\",theano.config.floatX))\n",
    "                             for mem in agent.agent_states]\n",
    "    t = 0\n",
    "    while True:\n",
    "        agent_output = step([observation],*prev_memories)\n",
    "        action,new_memories = agent_output[0],agent_output[1:]\n",
    "        \n",
    "        observation, reward, done, info = subm_env.step(action[0])\n",
    "        \n",
    "        prev_memories = new_memories\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        t+=1\n",
    "\n",
    "subm_env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./records1/openaigym.video.2.9510.video000008.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "#select the one you want\n",
    "video_path=\"./records1/openaigym.video.2.9510.video000008.mp4\"\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
